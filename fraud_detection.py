# -*- coding: utf-8 -*-
"""fraud-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cU69wOpO0Mxbp4LzjzHzEganSwEkD-oa
"""

# Core
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# ML tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Warnings
import warnings
warnings.filterwarnings("ignore")

# Step 0: Upload and unzip dataset.zip

from zipfile import ZipFile
import os

# If running in Colab, first upload the zip
from google.colab import files

uploaded = files.upload()  # Upload your dataset.zip here

# Extract the ZIP file
zip_file_name = "dataset.zip"  # Make sure this matches your uploaded file name

with ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall("data")  # Extract contents to 'data/' directory

# List extracted files
os.listdir("data")

import pandas as pd
import os

# Path where your .pkl files are stored
folder_path = "/content/data/data"

# List all .pkl files
pkl_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pkl')])

# Combine all into a single dataframe
df_list = []
for file in pkl_files:
    file_path = os.path.join(folder_path, file)
    df = pd.read_pickle(file_path)
    df_list.append(df)

# Final combined dataframe
df = pd.concat(df_list, ignore_index=True)

# Preview
df.head()

# Check shape and column types
print("Dataset shape:", df.shape)
df.info()

# Check for missing values
print("\nMissing values:\n", df.isnull().sum())

# Check target class distribution
df['TX_FRAUD'].value_counts().plot(kind='bar', title='Fraud vs Legitimate', xlabel='TX_FRAUD', ylabel='Count')

# Convert TX_TIME_SECONDS and TX_TIME_DAYS to integers
df['TX_TIME_SECONDS'] = pd.to_numeric(df['TX_TIME_SECONDS'], errors='coerce')
df['TX_TIME_DAYS'] = pd.to_numeric(df['TX_TIME_DAYS'], errors='coerce')

# Step 4: Feature Engineering

# 1. Convert datetime column
df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])

# 2. Convert TX_TIME_SECONDS and TX_TIME_DAYS from object to numeric
df['TX_TIME_SECONDS'] = pd.to_numeric(df['TX_TIME_SECONDS'], errors='coerce')
df['TX_TIME_DAYS'] = pd.to_numeric(df['TX_TIME_DAYS'], errors='coerce')

# 3. Extract additional temporal features from TX_DATETIME
df['TX_HOUR'] = df['TX_DATETIME'].dt.hour
df['TX_DAY'] = df['TX_DATETIME'].dt.day
df['TX_WEEKDAY'] = df['TX_DATETIME'].dt.weekday

# 4. Drop columns not needed for modeling
df.drop(columns=['TRANSACTION_ID', 'TX_DATETIME', 'TX_FRAUD_SCENARIO'], inplace=True)

# 5. Encode categorical IDs (CUSTOMER_ID and TERMINAL_ID)
df['CUSTOMER_ID'] = df['CUSTOMER_ID'].astype('category').cat.codes
df['TERMINAL_ID'] = df['TERMINAL_ID'].astype('category').cat.codes

# 6. Final feature check
df.head()

from sklearn.model_selection import train_test_split

X = df.drop('TX_FRAUD', axis=1)
y = df['TX_FRAUD']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

!pip install xgboost imbalanced-learn

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (after scaling)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, stratify=y, random_state=42)

# Apply SMOTE only on training data
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

# Check class balance
print("Before SMOTE:\n", y_train.value_counts())
print("After SMOTE:\n", y_train_bal.value_counts())

from xgboost import XGBClassifier

xgb_model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

xgb_model.fit(X_train_bal, y_train_bal)

# Get probabilities for threshold tuning
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import precision_score, recall_score, f1_score

thresholds = np.arange(0.0, 1.01, 0.05)
print(f"{'Threshold':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}")
print("-" * 40)

for t in thresholds:
    preds = (y_pred_proba >= t).astype(int)
    p = precision_score(y_test, preds, zero_division=0)
    r = recall_score(y_test, preds, zero_division=0)
    f1 = f1_score(y_test, preds, zero_division=0)
    print(f"{t:<10.2f} {p:<10.2f} {r:<10.2f} {f1:<10.2f}")

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# Choose a threshold (after printing test table, pick best tradeoff)
threshold = 0.35  # <--- ðŸ” Change this value based on your test results
y_pred_thresh = (y_pred_proba >= threshold).astype(int)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred_thresh))
print("\nClassification Report (Threshold = 0.35):\n", classification_report(y_test, y_pred_thresh))

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred_thresh), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title(f"Confusion Matrix (Threshold = {threshold})")
plt.show()

# ROC AUC
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC AUC Score: {roc_auc:.4f}")

import joblib

joblib.dump(xgb_model, "fraud_model.pkl")
joblib.dump(scaler, "scaler.pkl")